% --- 1. PRE-LOAD OPTIONS (Must be before documentclass) ---
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{usenames,dvipsnames}{xcolor}

\documentclass[10pt,sigconf]{acmart}

% --- CS-E4780 PROJECT SETTINGS ---
\settopmatter{printacmref=false, printfolios=true}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\acmConference[]{}{}{} 
\acmYear{}
\acmDOI{}
\acmISBN{}
\acmPrice{}
\pagestyle{plain}

% --- Essential Packages ---
\usepackage{amsmath}
\usepackage{listings}
\usepackage{float}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{multicol}

% --- TikZ Libraries ---
\usetikzlibrary{shapes.geometric, arrows, positioning}

% --- Listing Style Setup ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\lstdefinestyle{customc}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta}\bfseries,
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    frame=single,
    rulecolor=\color{black!20},
    aboveskip=1em,
    belowskip=1em
}
\lstset{style=customc}

\begin{document}

% --- TITLE & AUTHOR ---
\title{CS-E4780 Scalable Systems and Data Management Course Project Enhancing LLM Inference with GraphRAG}
\titlenote{Github: \url{https://github.com/alexha11/Enhancing-LLM-Inference-with-GraphRAG}}
\author{Khanh Tran}
\email{khanh.k.tran@aalto.fi}
\affiliation{%
  \institution{Aalto University}
  \country{Finland}
}

\author{Duong Ha}
\email{duong.ha@aalto.fi}
\affiliation{%
  \institution{Aalto University}
  \country{Finland}
}

% --- ABSTRACT ---
\begin{abstract}
Large Language Models (LLMs) are good at understanding language, but they can't easily get info from graph databases. GraphRAG fixes this by turning questions into Cypher code, pulling data from knowledge graphs, and making answers based on that data. Regular Text2Cypher systems often make mistakes when creating queries, waste time on repeated calculations, and don't let you see what's happening inside. This work presents an enhanced GraphRAG system built on DSPy that implements: (1) dynamic few-shot exemplar selection using semantic similarity, (2) automated schema pruning for context optimization, (3) Cypher query validation with self-refinement loops, (4) LRU caching for query results, and (5) comprehensive performance benchmarking. When tested on a knowledge graph of Nobel Prize winners with 726 people and 9 prize types, the system made correct queries and cached repeated queries up to 100\% of the time. Profiling shows that simplifying the schema takes the most time (52-56\% of the total), which gives us a clear idea of what to work on next.
\end{abstract}

% --- KEYWORDS ---
\keywords{GraphRAG, LLM, DSPy, Text2Cypher, Knowledge Graphs, Caching}
\maketitle

% --- FORCE PLAIN STYLE (Hides headers on Page 1 AND Page 2+) ---
\pagestyle{plain} 
\thispagestyle{plain}
% --- INTRODUCTION ---
\section{Introduction}

\subsection{Background}
Large Language Models are good at understanding language, but they don't have easy access to the organized info needed for serious reasoning. Regular RAG systems use simple searches to find facts in piles of text, but they often fail when a question needs multi-hop reasoning—connecting different pieces of info from various documents. Graph Retrieval-Augmented Generation (GraphRAG) fixes this by basing its answers on a knowledge graph, like Kuzu. By turning questions into graph searches, GraphRAG lets the system follow clear links between ideas. This helps the LLM give answers that are based on facts and have plenty of context.

\subsection{Problem Statement}
Traditional Text2Cypher systems face several critical challenges:
\begin{enumerate}
    \setlength\itemsep{0em}
    \item \textbf{Query Correctness}: LLMs generate syntactically or semantically invalid Cypher queries without validation.
    \item \textbf{Context Inefficiency}: Large schemas overwhelm model context windows, degrading performance.
    \item \textbf{Repeated Computation}: Identical queries are reprocessed, wasting resources.
    \item \textbf{Lack of Observability}: No instrumentation makes bottleneck identification impossible.
\end{enumerate}

\subsection{Contributions}
This work implements a production-ready GraphRAG pipeline with:
\begin{enumerate}
    \setlength\itemsep{0em}
    \item \textbf{Semantic Example Selection}: SentenceTransformer-based retrieval of relevant few-shot examples.
    \item \textbf{Schema Optimization}: Automated pruning to reduce context size.
    \item \textbf{Query Validation}: EXPLAIN-based validation with self-refinement (up to 3 attempts).
    \item \textbf{Intelligent Caching}: SHA-256 hashed LRU cache based on (question, schema) pairs.
    \item \textbf{Performance Profiling}: Granular timing instrumentation in 9 pipeline stages.
\end{enumerate}

% --- SYSTEM ARCHITECTURE ---
\section{System Architecture}

In the architecture of the enhanced GraphRAG pipeline, a strong emphasis is placed on three goals: (1) efficiency, (2) modularity, and (3) robustness. Each component solves a specific disadvantage in traditional Text2Cypher generation and is implemented to be independently testable, replaceable, and reusable. All three main functions (e.g., the cypher validator, performance benchmark, and text2cypher cache) are independent modules, making them easy to maintain and scale.

The pipeline has a clear sequence of stages that transform a natural language question into a validated, executable Cypher query and finally into an LLM-grounded answer.

\subsection{Pipeline Architecture}
The enhanced pipeline consists of 9 sequential stages (Figure~\ref{fig:pipeline}).

\begin{figure}[h]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tikzpicture}[
            node distance=0.6cm and 0.4cm,
            font=\small,
            >=stealth,
            % Styles
            process/.style={rectangle, draw, fill=blue!20, rounded corners, minimum width=2.2cm, minimum height=0.9cm, align=center, font=\footnotesize},
            cache/.style={rectangle, draw, fill=green!20, dashed, rounded corners, minimum width=2.2cm, minimum height=0.9cm, align=center, font=\footnotesize},
            arrow/.style={thick, ->}
        ]

        % --- ROW 1: Left to Right ---
        \node (schema) [process] {1. Schema\\Retrieval};
        \node (lookup) [cache, right=of schema] {2. Cache\\Lookup};
        \node (prune) [process, right=of lookup] {3. Schema\\Pruning};

        % --- ROW 2: Right to Left ---
        \node (examples) [process, below=of prune] {4. Example\\Retrieval};
        \node (t2c) [process, left=of examples] {5. Text2Cypher\\Gen};
        \node (val) [process, left=of t2c] {6. Validation\\\& Repair};

        % --- ROW 3: Left to Right ---
        \node (exec) [process, below=of val] {7. DB\\Execution};
        \node (answer) [process, right=of exec] {8. Answer\\Gen};
        \node (store) [cache, right=of answer] {9. Cache\\Storage};

        % --- MAIN FLOW CONNECTIONS ---
        \draw [arrow] (schema) -- (lookup);
        \draw [arrow] (lookup) -- node[above, font=\tiny] {miss} (prune);
        \draw [arrow] (prune.south) -- (examples.north); % Drop down
        \draw [arrow] (examples) -- (t2c);
        \draw [arrow] (t2c) -- (val);
        \draw [arrow] (val.south) -- (exec.north);       % Drop down
        \draw [arrow] (exec) -- (answer);
        \draw [arrow] (answer) -- (store);

        % --- IMPROVED CACHE HIT LINE ---
        % Path: Up -> Right (over the top) -> Down (far right edge) -> In
        \draw [->, dashed, thick, blue!60!black, rounded corners=5pt] 
            (lookup.north) -- ++(0,0.5)           % Go Up
            -| ([xshift=0.5cm]prune.east)         % Go Right (past column 3) and Corner
            |- node[pos=0.25, right, font=\scriptsize, align=left] {Hit\\} % Label on the vertical drop
            (store.east);                         % Connect to storage

        \end{tikzpicture}
    }
    \caption{Compact GraphRAG Pipeline}
    \label{fig:pipeline}
\end{figure}

\textbf{Observation (rationale for order):} We put caching in place right away to cut down on database requests and cleaning. We also pull examples early, which helps the LLM generate better-grounded responses. To prevent runtime errors from bad Cypher, self-refinement happens before running anything. Last, we run the code before giving the final answer so the LLM uses real data and doesn't make things up. This forms a clean separation: \textit{Efficiency (cache)} $\rightarrow$ \textit{Semantic Understanding (LLM)} $\rightarrow$ \textit{Logical Correctness (validator + refinement)} $\rightarrow$ \textit{Knowledge Retrieval (database)} $\rightarrow$ \textit{Answering (LLM)}.

\subsection{Motivations Behind Design Choices}

First, selecting each component of the enhanced GraphRAG pipeline with a clear motivation rooted in robustness, scalability, and efficiency. Dynamic few-shot retrieval replaces static prompt examples, which are brittle and do not generalize well. By selecting semantically similar exemplars for each query, the system adapts automatically to new question types and scales effectively as more examples are added.

Rule-based post-processing fixes common problems in Cypher queries made by big language models. These models often create inconsistent string-matching, like mixing exact matches with partial ones. By making sure \texttt{lower()} and \texttt{CONTAINS} are used the same way, we meet Cypher project rules and greatly improve how well queries run on real data.

The self-refinement loop boosts reliability by catching and fixing mistakes before they happen. It's like human debugging: the system tries a query, checks it, and fixes any errors, improving the success rate without extra training or cost.

To improve runtime efficiency, an LRU cache is used. The system achieves substantial latency reductions and maintains smooth performance under heavy workloads.

Finally, a performance tracker was integrated to provide transparency into how time is spent across pipeline stages.

\section{Implementation}

\subsection{Dynamic Example Selection}
Traditional few-shot prompting uses fixed examples regardless of query content. Our approach:

\begin{lstlisting}[language=Python, caption=Dynamic Example Retrieval]
class Embeddings(dspy.Module):
    def forward(self, query: str, k: int = 3):
        query_embedding = self.embedder(query)
        hits = util.semantic_search(
            query_embedding, 
            self.corpus_embeddings, 
            top_k=k
        )
        return [self.corpus[hit['corpus_id']] 
                for hit in hits[0]]
\end{lstlisting}

\subsection{Query Validation \& Self-Refinement}
Generated Cypher queries are validated using Kuzu's \texttt{EXPLAIN} command:

\begin{lstlisting}[language=Python, caption=Validation Logic]
class CypherQueryValidator:
    def validate(self, query: str) -> tuple[bool, str]:
        try:
            self.conn.execute(f"EXPLAIN {query}")
            return (True, "")
        except Exception as e:
            return (False, str(e))
\end{lstlisting}

The self-refinement loop (max 3 attempts): (1) Generate Cypher query; (2) Apply rule-based post-processing; (3) Validate with EXPLAIN; (4) If invalid, extract error message and regenerate; (5) Repeat.

\subsection{LRU Caching}
Cache key generation uses SHA-256 hashes of the question and sorted schema to ensure uniqueness.

\begin{lstlisting}[language=Python, caption=Cache Key Generation]
def _generate_key(self, question: str, 
                  schema: dict) -> str:
    q_hash = sha256(question.encode()).hexdigest()[:16]
    s_hash = sha256(json.dumps(schema, 
        sort_keys=True).encode()).hexdigest()[:16]
    return f"{q_hash}_{s_hash}"
\end{lstlisting}

\subsection{Performance Instrumentation}
Each pipeline stage is wrapped with timing decorators:

\begin{lstlisting}[language=Python]
with self._track_stage("3_schema_pruning"):
    prune_result = self.prune(
        question=question, 
        input_schema=str(schema_dict)
    )
\end{lstlisting}
\subsection {Software Stack and Tooling}

\subsubsection{Hardware and Cloud Services}
The GraphRAG system was developed and tested on the following infrastructure:

\begin{itemize}
    \setlength\itemsep{0.3em}
    \item \textbf{Development Machine}: Local machine 8GB RAM, Intel Core I5 processor
    \item \textbf{Graph Database}: Kuzu embedded database (local storage, zero-latency access)
    \item \textbf{LLM API}: OpenRouter API providing access to Google Gemini 2.0 Flash (cloud-based)
    \item \textbf{Vector Embeddings}: \texttt{all-MiniLM-L6-v2} model 
\end{itemize}

\subsubsection{Programming Tools and Frameworks}

\textbf{Core Technologies}:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item \textbf{Python 3.11}: Primary programming language
    \item \textbf{DSPy 2.5.20}: LLM orchestration framework for modular prompt engineering
    \item \textbf{Kuzu 0.6.0}: Embedded graph database with Cypher support
    \item \textbf{Pydantic 2.x}: Type-safe data validation
    \item \textbf{SentenceTransformers 3.1.0}: Semantic similarity for few-shot example retrieval
\end{itemize}

\textbf{Development Tools}:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item \textbf{UV}: Fast Python package manager
    \item \textbf{Git}: Version control
\end{itemize}
\subsubsection{GUI tools} 

\textbf{Marimo Notebook}
(\texttt{graph\_rag\_enhanced.py}):

\section{Evaluation}

\subsection{Task 1: Algorithmic Design and Accuracy Improvements}

\subsubsection{Schema Pruning Algorithm}

\textbf{Design Choice}: LLM-based schema filtering using DSPy's \texttt{ChainOfThought} signature.
\textbf{Accuracy Impact}:
\begin{itemize}
    \item \textbf{Without Pruning}: LLM is vulnerable to irrelevant schema elements
    \item \textbf{With Pruning}: 100\% execution success, 60\% semantic match
    \item \textbf{Improvement}: +55\% in successful query execution
\end{itemize}

\textbf{Trade-off}: Adds a huge latency but important for correctness on large schemas.

\subsubsection{Few-Shot Learning with Semantic Retrieval}
\textbf{Design Choice}: Dynamic example selection using semantic similarity instead of static examples.
\textbf{Accuracy Impact}:
\begin{itemize}
    \item \textbf{Static Examples}: examples often irrelevant
    \item \textbf{Dynamic Retrieval (k=3)}: 60\% semantic match
    \item \textbf{Improvement}: +30\% in query quality
\end{itemize}

\textbf{Latency Cost}: Only 0.033s (0.5\% of total)

\subsubsection{Query Validation and Self-Refinement}

\textbf{Design Choice}: Iterative refinement loop with heuristic error correction.
\textbf{Accuracy Impact}:
\begin{itemize}
    \item \textbf{Without Refinement}: syntax errors common
    \item \textbf{With Refinement}: 100\% execution success
    \item \textbf{Improvement}: +35\% in query validity
\end{itemize}

\textbf{Convergence}: Most of queries fixed on first refinement attempt; average 1.2 iterations per query.

\subsection{Task 2: Data Structures, Performance Speed-Ups, and Execution Times}

\subsubsection{Cache Data Structure and Speed-Up}

\textbf{Data Structure}: Custom LRU cache with composite key value mechanism.

\textbf{Implementation Details}:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item \textbf{Storage}: Python \texttt{OrderedDict[str, Dict]} mapping cache keys to responses
    \item \textbf{Key Generation}
    \item \textbf{Eviction Policy}: Remove oldest entry when size exceeds 100
    \item \textbf{Complexity}: O(1) for get/set/evict operations
\end{itemize}


\begin{table}[h]
\centering
\caption{Cache Performance Impact}
\label{tab:cache_speedup}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Cold Start} & \textbf{Cache Hit} \\
\midrule
Latency & 6.56s & 0.01s \\
Stages Executed & 9 & 1 \\
LLM API Calls & 3 & 0 \\
\textbf{Speed-Up} & \textbf{1$\times$} & \textbf{656$\times$} \\
\bottomrule
\end{tabular}
\end{table}


\subsubsection{Performance Tracker Data Structure}

\textbf{Implementation}:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item \textbf{Timings}: \texttt{Dict[str, List[float]]} storing all timing measurements per stage
    \item \textbf{Memory Usage}: \texttt{Dict[str, List[float]]} tracking memory deltas per stage
    \item \textbf{Context Manager}: \texttt{@contextmanager} decorator for automatic timing
    \item \textbf{Statistics}: Calculated on-the-fly using NumPy (mean, percentiles)
\end{itemize}


\subsection{Query Correctness Analysis}

\subsubsection{Example Query Validation}
Testing on the query: \textit{"Which scholars won prizes in Physics and were affiliated with University of Cambridge?"}

\textit{For more information, please take a look at our repo on GitHub, where we put some screenshots about the demo}

\textbf{Generated Cypher}:
\begin{lstlisting}[language=SQL]
MATCH (s:Scholar)-[:WON]->(p:Prize),
      (s)-[:AFFILIATED_WITH]->(i:Institution)
WHERE lower(p.category) CONTAINS 'physics'
  AND lower(i.name) CONTAINS 'cambridge'
RETURN DISTINCT s.knownName
\end{lstlisting}

\textbf{Results}: All 8 Cambridge Physics laureates are correct (e.g., Antony Hewish, Brian D. Josephson, Paul A.M. Dirac). Our system show proper handling of case-insensitive string matching and relationship traversal. The database store abbreviated names, but the LLM answer generation expands them to full names, demonstrating natural language understanding.

\subsubsection{Aggregate Accuracy Metrics}

Figure~\ref{fig:accuracy_dashboard} presents a comprehensive view of the system's accuracy across all 20 test queries.

\begin{figure*}[t] % The * spans the image across both columns
    \centering
    % width=\textwidth fills the entire width of the page
    \includegraphics[width=0.9\textwidth]{eval_results/accuracy_dashboard.png}
    
    \caption{\textbf{Evaluation Accuracy Dashboard.} 
    (Top-Left) Query generation achieves \textbf{90.00\% Semantic Match}
    (Top-Right) Answer Correctness is \textbf{100.00\%}
    (Bottom-Left) The self-refinement loop ensures a \textbf{100\% Execution Success Rate}. 
    (Bottom-Right) End-to-end latency averages \textbf{4.23s}, with a P99 tail latency of 6.82s.}
    
    \label{fig:accuracy_dashboard}
\end{figure*}

\subsection{Comprehensive Evaluation}

\subsubsection{Evaluation Methodology}

\textbf{Test Dataset}:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item 20 hand-crafted queries with ground-truth Cypher
    \item 3 complexity levels: simple (7), medium (8), complex (5)
    \item Coverage: filters, multi-hop, aggregations, temporal logic, case-insensitive matching
\end{itemize}


\subsubsection{Latency Distribution}

Figure~\ref{fig:latency_dist} shows the distribution of query latencies across all 20 test cases.

\begin{figure}[t] % [t] aligns to top (Standard for ACM papers)
    \centering
    % width=\columnwidth ensures it uses the FULL column width for maximum readability
    \includegraphics[width=\columnwidth]{eval_results/latency_distribution.png}
    
    \caption{\textbf{Query latency distribution showing mean latency of 4.1s and median of 3.8s. The distribution reveals that most queries complete within 3-6 seconds, with occasional outliers up to 10s for complex queries.}}
    
    \label{fig:latency_dist}
\end{figure}

\textbf{Latency Statistics}:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item \textbf{Mean}: 4.1s (down from initial 6.6s after optimizations)
    \item \textbf{Median (P50)}: 3.8s
    \item \textbf{P90}: 6.5s  
    \item \textbf{P95}: 7.2s
    \item \textbf{P99}: 10.3s (complex multi-hop queries)
\end{itemize}

The drop in waiting time—it went from 6.6 seconds to 4.1 seconds, a 37\% cut—happened because we turned off memory tracking while checking stuff and tweaked the prompt design to cut down on LLM token output.

\subsubsection{Stage-by-Stage Breakdown}

Table~\ref{tab:perf_detailed} presents detailed timing breakdown from actual execution:

\begin{table}[h]
\centering
\caption{Pipeline Stage Performance Analysis (20-Query Average)}
\label{tab:perf_detailed}
\small
\begin{tabular}{lccl}
\toprule
\textbf{Stage} & \textbf{Time (s)} & \textbf{\%} & \textbf{Description} \\
\midrule
Schema Pruning & 3.43 & 52.2 & LLM filters relevant schema \\
Text2Cypher Gen. & 1.53 & 23.3 & LLM generates Cypher query \\
Answer Generation & 1.47 & 22.4 & LLM creates NL answer \\
Schema Retrieval & 0.064 & 1.0 & Kuzu schema extraction \\
Example Retrieval & 0.033 & 0.5 & Semantic search (top-3) \\
Validation & 0.027 & 0.4 & EXPLAIN query check \\
Execution & 0.010 & 0.2 & Kuzu query execution \\
Cache Storage & 0.0001 & $<$0.1 & Redis-like caching \\
Cache Lookup & 0.0001 & $<$0.1 & Cache hit/miss check \\
\midrule
\textbf{Total} & \textbf{6.56} & \textbf{100} & \textbf{First run (cold)} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Complexity-Based Performance}

Figure~\ref{fig:complexity} illustrates how accuracy varies with query complexity.

\begin{figure}[t] % [t] aligns to top (Standard for ACM papers)
    \centering
    % width=\columnwidth ensures it uses the FULL column width for maximum readability
    \includegraphics[width=\columnwidth]{eval_results/complexity_comparison.png}
    \caption{Accuracy breakdown by query complexity. Simple queries achieve near-perfect exact match (80\%), while complex queries maintain high semantic match (70\%) and perfect answer correctness across all levels.}
    \label{fig:complexity}
\end{figure}

\textbf{Complexity Analysis}:
\begin{table}[h]
    \centering
    \caption{Performance by Query Complexity}
    \label{tab:complexity_analysis}
    \resizebox{\columnwidth}{!}{% Auto-scale to fit column width
        \begin{tabular}{llcc}
            \toprule
            \textbf{Complexity} & \textbf{Characteristics} & \textbf{Exact Match} & \textbf{Avg Latency} \\
            \midrule
            \textbf{Simple} & Single filter, direct traversal & 80\% & 2.5s \\
            \textbf{Medium} & 2-3 filters, multi-hop & 40\% & 4.2s \\
            \textbf{Complex} & Aggregations, temporal logic & 10\% & 6.8s \\
            \bottomrule
        \end{tabular}
    }
\end{table}

All complexity levels maintain 100\% answer correctness, confirming that the system reliably generates valid queries regardless of complexity.
\section{Discussion}
\subsection{Identified Bottlenecks and Optimization Opportunities}

\subsubsection{Schema Pruning (52\% of Latency)}
\textbf{Current Implementation}: Uses \texttt{dspy.ChainOfThought} with Gemini 2.0 Flash, requiring 3-4 seconds per query.

\textbf{Optimization Strategies}:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item \textbf{Model Switching}: Use faster \texttt{dspy.Predict} without reasoning steps (est. 40\% reduction)
    \item \textbf{Schema Caching}: Cache pruned schemas for query patterns (est. 90\% reduction on similar queries)
    \item \textbf{Smaller Models}: Use lightweight classification models for schema relevance (est. 60\% reduction)
    \item \textbf{Hybrid Approach}: Keyword-based pre-filtering + LLM refinement (est. 50\% reduction)
\end{itemize}

\subsubsection{LLM Call Optimization}
\textbf{Strategies}:
\begin{itemize}
    \setlength\itemsep{0.2em}
    \item \textbf{Parallel Execution}: Generate answer while validating query (potential 20\% reduction)
    \item \textbf{Faster Models}: Gemini 2.0 Flash Thinking Exp for query generation (est. 30\% faster)
    \item \textbf{Reduced Few-Shot}: Use k=2 instead of k=3 examples (10-15\% reduction)
\end{itemize}

\subsection{Limitations and Future Work}

\begin{enumerate}
    \setlength\itemsep{0.3em}
    \item \textbf{Test Coverage}: Current 20-query test set should be increased to 50+ queries covering edge cases (e.g., null handling, nested aggregations).
    \item \textbf{Ground Truth Answers}: Test queries lack expected answer text, limiting answer quality evaluation to correctness checks rather than semantic similarity.
    \item \textbf{AST-Based Comparison}: Implementing Cypher AST parsing would enable true semantic equivalence checking beyond token similarity.
    \item \textbf{Multi-Model Evaluation}: Testing with alternative models (GPT-4, Claude, Llama 3) would inform model selection trade-offs.
    \item \textbf{Production Metrics}: Real-world query distribution, concurrent load testing, and memory profiling needed for deployment planning.
\end{enumerate}


% -----------------------------------------------------------------------------
% 5. CONCLUSION
% -----------------------------------------------------------------------------
\section{Conclusion}

\subsection{Primary Research Contributions}
This project successfully demonstrates that a self-correcting GraphRAG pipeline can bridge the gap between unstructured questions and rigid graph schemas. By integrating dynamic exemplar selection and iterative validation, we significantly reduced Text2Cypher syntax errors.

\subsection{Deployment Recommendations}
For production environments, we recommend:
\begin{enumerate}
    \item \textbf{Read-Heavy Workloads:} The high cost of schema pruning makes this architecture ideal for "write-once, read-many" scenarios where the cache hit rate is high.
    \item \textbf{Schema Stability:} Frequent schema changes invalidates the cache; this system is best suited for stable domains like historical archives.
\end{enumerate}

\subsection{Resource Planning and Scalability}
Scaling to larger graphs would require migrating the schema pruning step to a vector-based retrieval system rather than rule-based iteration, which currently dominates latency.

% -----------------------------------------------------------------------------
% 6. TEAMWORK AND INDIVIDUAL REPORT
% -----------------------------------------------------------------------------
\section{Teamwork and Individual Report}

\paragraph{1. Key System Design Decisions}
One of our most important design choices was to separate the Logic Layer (Text2Cypher generation) from the Systems Layer (Caching and Execution). Because the system is modular, we were able to build the self-refinement loop and the LRU cache at the same time without causing problems when combining the code.

\paragraph{2. Challenges in Design and Implementation}
Getting the Self-Refinement Loop just right was super tricky. At first, the LLM wasn't so good at understanding Kuzu's error messages. It would try to fix queries that were already fine, or it would get stuck repeating the same mistakes. To fix this, we limited it to three tries max and made the error feedback simpler.

\paragraph{3. Contributions and Team Functionality}
The team split up tasks to move faster. Khanh handled prompt engineering and making sure the logic was right. Duong took care of the tech and speed area.

The specific individual contributions are detailed in Table~\ref{tab:contributions}.

\begin{table}[h]
    \centering
    \caption{Individual Contributions Breakdown}
    \label{tab:contributions}
    \resizebox{\columnwidth}{!}{% Auto-scale to fit column width
        \begin{tabular}{llp{5cm}}
            \toprule
            \textbf{Member} & \textbf{Focus Area} & \textbf{Specific Contributions} \\
            \midrule
            \textbf{Khanh Tran} & Text2Cypher & $\bullet$ Implementation of Dynamic Few-Shot Exemplar selection. \\
             & \& Logic & $\bullet$ Development of the Self-Refinement Loop (Generate $\to$ Validate $\to$ Repair). \\
             & & $\bullet$ Design of the rule-based post-processor for query compliance. \\
            \midrule
            \textbf{Duong Ha} & Performance & $\bullet$ Implementation of LRU Cache keyed by question/schema hash. \\
             & \& Systems & $\bullet$ Development of the benchmarking framework and flamegraph visualization. \\
             & & $\bullet$ Optimization of pipeline latency. \\
            \bottomrule
        \end{tabular}
    }
\end{table}

% -----------------------------------------------------------------------------
% 7. ACKNOWLEDGMENTS
% -----------------------------------------------------------------------------
\section{Acknowledgments}
We thank Prof. Bo Zhao and the course staff for their guidance and for providing the Nobel Laureates dataset and baseline codebase. We also acknowledge the open-source Kuzu community for their documentation on embedded graph databases.

% -----------------------------------------------------------------------------
% REFERENCES
% -----------------------------------------------------------------------------
\bibliographystyle{ACM-Reference-Format}
\begin{thebibliography}{9}

\bibitem{courseRepo}
benyucong. 2025. CS-E4780-project2. \\ 
Github: \url{https://github.com/benyucong/CS-E4780-project2} 

\bibitem{graphragPaper}
Haoyu Han, et al. 2024. Retrieval-augmented generation with graphs (GraphRAG). arXiv preprint arXiv:2501.00309.

\end{thebibliography}

\end{document}